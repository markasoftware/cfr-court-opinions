* Mark Polyakov's eCFR analyzer
  A web tool to see interesting facts about the Code of Federal Regulations!

  My implementation cross-correlates with US Court opinions
  [[https://www.govinfo.gov/app/collection/uscourts][available on GovInfo]] to display how often
  each title/part/subpart of the CFR is referenced from court opinions, and to browse the individual
  court opinions associated with each.

  I run a hosted version of this tool at https://markasoftware.com/cfr with all 2024 court opinions.
  It's possible (and beneficial!) to run it with more data, but it takes a while to do so because
  each year has hundreds of thousands of PDFs of court opinions that must be processed, and the
  GovInfo API does not support bulk downloading of court opinions.
* Running
** Prerequisites
   + The ~uv~ Python package manager should be installed to your ~$PATH~.
   + A relatively modern ~curl~ version
** Downloading Court Opinion PDFs
   This is much more involved than scraping the eCFR data, because each opinion's PDFs have to be
   scraped separately. There is some intentional rate limiting on the govinfo side, but that's not
   the real problem; their servers seem unable to handle downloading huge volumes of PDFs and start
   giving 503s somewhat frequently.

   1. Get an API key from govinfo
   2. Run ~uv run pdfs.py --api-key YOUR_API_KEY --year 2024 --month 4 --work-dir
      ~/tmp/ecfr-work-dir~ for example to scrape info from all pdfs into the given working dir.
      The scraper saves its progress; if you interrupt and resume it, it will pick up where it left
      off. It is recommended and safe to scrape multiple years and months to the same work dir (files
      are put in a subdirectory for the selected year/month).

      You should parallelize across years/months that you wish to download using an external tool, eg
      GNU Parallel.
** Downloading eCFR data
   Just run ~uv run ecfrs.py --work-dir ~/tmp/ecfr-work-dir~.

   Ideally we'd download the XML for each title individually...however specifically the XML for
   title 40 seems to be too big to download, their server consistently times out. So instead, the
   script downloads the XML for each part separately. This means it takes quite a while (few hours).
** Parsing the raw data
   Both the download scripts listed above do the minimal amount of work that requires network
   access. Next, you must convert the downloaded data to an SQLite database that the server can use.
   To do this, run: ~uv run parse_work_dir.py --work-dir ~/tmp/ecfr-work-dir --database
   /path/to/database.sqlite~ (the database file will be created, or overwritten if it already
   exists)
** Running the server
   The frontend is entirely a static site! This is possible thanks to sqlite-wasm: We load the
   sqlite database into the browser when the page is first loaded then query it with javascript from
   then on.

   So, all you have to do is build the frontend: ~parcel index.html~
