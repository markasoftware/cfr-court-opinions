* Mark Polyakov's eCFR - Court Opinion cross reference finder
  A web tool to see interesting facts about the Code of Federal Regulations and how it's referenced
  from court opinions!

  My implementation cross-correlates with US Court opinions [[https://www.govinfo.gov/app/collection/uscourts][available on GovInfo]] to display how
  often each title/part/subpart of the CFR is referenced from court opinions, and to browse the
  individual court opinions associated with each.

  I run a hosted version of this tool at https://markasoftware.com/cfr-court-opinions with all 2024
  court opinions. It's possible (and beneficial!) to run it with more data, but it takes a while to
  do so because each year has hundreds of thousands of PDFs of court opinions that must be
  processed, and the GovInfo API does not support bulk downloading of court opinions so there's a
  lot of HTTP requests involved.
* Running
** Prerequisites
   + The ~uv~ Python package manager
   + ~npm~ and node.js
   + A relatively modern ~curl~ version
** Downloading Court Opinion PDFs
   This is much more involved than scraping the eCFR data, because each opinion's PDFs have to be
   scraped separately. There is some intentional rate limiting on the govinfo side, but that's not
   the real problem; their servers seem unable to handle downloading huge volumes of PDFs and start
   giving 503s somewhat frequently.

   1. Get an API key from govinfo
   2. Run ~uv run pdfs.py --api-key YOUR_API_KEY --year 2024 --month 4 --work-dir
      ~/tmp/ecfr-work-dir~ for example to scrape info from all pdfs into the given working dir.
      The scraper saves its progress; if you interrupt and resume it, it will pick up where it left
      off. It is recommended and safe to scrape multiple years and months to the same work dir (files
      are put in a subdirectory for the selected year/month).

      You should parallelize across years/months that you wish to download using an external tool, eg
      GNU Parallel.
** Downloading eCFR data
   Just run ~uv run ecfrs.py --work-dir ~/tmp/ecfr-work-dir~.

   Ideally we'd download the XML for each title individually...however specifically the XML for
   title 40 seems to be too big to download, their server consistently times out. So instead, the
   script downloads the XML for each part separately. This means it takes quite a while (few hours).
** Parsing the raw data
   Both the download scripts listed above do the minimal amount of work that requires network
   access. Next, you must convert the downloaded data to an SQLite database that the server can use.
   To do this, run: ~uv run make_database.py --work-dir ~/tmp/ecfr-work-dir --database
   /path/to/repo/cfr-db.sqlite~ (the database file will be created, or overwritten if it already
   exists). You must use the filename ~cfr-db.sqlite~ and place it in this repo's directory for the
   next step to be able to find it.
** Running the server
   The frontend is entirely a static site! This is possible thanks to sqlite-wasm: We load the
   sqlite database into the browser when the page is first loaded then query it with javascript from
   then on.

   So, all you have to do is ~npm run serve~ to get a live server running.
